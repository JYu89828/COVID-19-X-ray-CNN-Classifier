{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "X-Ray_CNN_FinalProject.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgGSTGrPANGb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2c8d387-8d4f-48ac-bedd-582226f27845"
      },
      "source": [
        "from google.colab import files, drive\r\n",
        "\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TCbwCc8Aaky"
      },
      "source": [
        "#Import needed packages\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torchvision.transforms import transforms\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from torch.optim import Adam\r\n",
        "from torch.autograd import Variable\r\n",
        "import numpy as np\r\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdqUAarDHstC"
      },
      "source": [
        "Define the network structure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsEYFgc8a9pF"
      },
      "source": [
        "\r\n",
        "\r\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\r\n",
        "\r\n",
        "class Net(nn.Module):\r\n",
        "    def __init__(self, num_classes=7):\r\n",
        "        super(Net, self).__init__()\r\n",
        "\r\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\r\n",
        "        self.relu1 = nn.ReLU()\r\n",
        "\r\n",
        "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=12, kernel_size=3, stride=1, padding=1)\r\n",
        "        self.relu2 = nn.ReLU()\r\n",
        "\r\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2)\r\n",
        "\r\n",
        "        self.conv3 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\r\n",
        "        self.relu3 = nn.ReLU()\r\n",
        "\r\n",
        "        self.conv4 = nn.Conv2d(in_channels=24, out_channels=24, kernel_size=3, stride=1, padding=1)\r\n",
        "        self.relu4 = nn.ReLU()\r\n",
        "\r\n",
        "        self.fc = nn.Linear(in_features=16 * 16 * 24, out_features=num_classes)\r\n",
        "\r\n",
        "    def forward(self, input):\r\n",
        "        output = self.conv1(input)\r\n",
        "        output = self.relu1(output)\r\n",
        "\r\n",
        "        output = self.conv2(output)\r\n",
        "        output = self.relu2(output)\r\n",
        "\r\n",
        "        output = self.pool(output)\r\n",
        "\r\n",
        "        output = self.conv3(output)\r\n",
        "        output = self.relu3(output)\r\n",
        "\r\n",
        "        output = self.conv4(output)\r\n",
        "        output = self.relu4(output)\r\n",
        "\r\n",
        "        output = output.view(-1, 16 * 16 * 24)\r\n",
        "\r\n",
        "        output = self.fc(output)\r\n",
        "\r\n",
        "        return output"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph5L0SiHHwrq"
      },
      "source": [
        "Import the raw images, with necessary resize and convert into standarzied RGB *format*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5dzNfSyBMxC"
      },
      "source": [
        "\r\n",
        "#create customized dataset\r\n",
        "import os\r\n",
        "import glob\r\n",
        "import numpy as np\r\n",
        "from skimage import io\r\n",
        "import re\r\n",
        "from PIL import Image\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "import re\r\n",
        "\r\n",
        "class Lung_Pnemonia(Dataset):\r\n",
        "\r\n",
        "  def __init__(self,dir,transform=None):\r\n",
        "    self.dir=dir\r\n",
        "    self.transform=transform\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    files=glob.glob(self.dir+'/*.jpeg')[:1000]\r\n",
        "    #files.extend(glob(self.dir+'/*.png'))\r\n",
        "    #files.extend(glob(self.dir+'/*.jpg'))\r\n",
        "    return len(files)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "  def __getitem__(self,idx):\r\n",
        "    if torch.is_tensor(idx):\r\n",
        "      idx=idx.tolist()\r\n",
        "\r\n",
        "    all_instances=glob.glob(self.dir+'/*.jpeg')[:1000] #return a list of file names\r\n",
        "    read = lambda imname: np.asarray(Image.open(imname).resize((32,32), resample = Image.BILINEAR).convert(\"RGB\")) \r\n",
        "    ims = read(os.path.join(self.dir, all_instances[idx])) \r\n",
        "    im_array=np.array(ims)   \r\n",
        "    #print(all_instances[idx])\r\n",
        "    if bool(re.search(('IM'), all_instances[idx])):\r\n",
        "      label=np.array(0)\r\n",
        "    elif bool(re.search(('virus'), all_instances[idx])):\r\n",
        "      label=np.array(1)\r\n",
        "    elif bool(re.search(('bacteria'), all_instances[idx])):\r\n",
        "      label=np.array(2)\r\n",
        "    elif bool(re.search(('streptococcus'), all_instances[idx])):\r\n",
        "      label=np.array(3)\r\n",
        "    elif bool(re.search(('acute'), all_instances[idx])):\r\n",
        "      label=np.array(4)\r\n",
        "    elif bool(re.search(('SARS'), all_instances[idx])):\r\n",
        "      label=np.array(5)\r\n",
        "    else:\r\n",
        "      label=np.array(6)\r\n",
        "\r\n",
        "    instance={'image':im_array, 'label':label}      \r\n",
        "    if self.transform:\r\n",
        "      instance['image']= self.transform(instance['image'])\r\n",
        "\r\n",
        "    return instance\r\n",
        "    "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imPwiyHHKDoJ"
      },
      "source": [
        "create train/test data loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kAPuLIDBSni"
      },
      "source": [
        "#create train/val data loader\r\n",
        "from torch.utils.data import random_split\r\n",
        "from torchvision import transforms, utils\r\n",
        "\r\n",
        "batch_size=28\r\n",
        "\r\n",
        "trainset=Lung_Pnemonia('/content/drive/My Drive/Coronahack-Chest-XRay-Dataset/train',\r\n",
        "                       transform=transforms.Compose([transforms.ToTensor(),\r\n",
        "                                                     transforms.Normalize((0.5,), (0.5,))]))\r\n",
        "\r\n",
        "train_loader=DataLoader(trainset,batch_size,shuffle=True,num_workers=1)\r\n",
        "\r\n",
        "testset=Lung_Pnemonia('/content/drive/My Drive/Coronahack-Chest-XRay-Dataset/test',transform=transforms.Compose([transforms.ToTensor(),\r\n",
        "                                                                                                                 transforms.Normalize((0.5,), (0.5,))]))\r\n",
        "\r\n",
        "test_loader=DataLoader(testset,batch_size,shuffle=True,num_workers=1)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lovYmXBOKHqx"
      },
      "source": [
        "define optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7o4ssw0lUaj"
      },
      "source": [
        "\r\n",
        "from torch.optim import Adam\r\n",
        "\r\n",
        "\r\n",
        "# Check if gpu support is available\r\n",
        "cuda_avail = torch.cuda.is_available()\r\n",
        "\r\n",
        "# Create model, optimizer and loss function\r\n",
        "model = Net(num_classes=7)\r\n",
        "\r\n",
        "#if cuda is available, move the model to the GPU\r\n",
        "if cuda_avail:\r\n",
        "    model.cuda()\r\n",
        "\r\n",
        "#Define the optimizer and loss function\r\n",
        "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\r\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1Vv7is9JcJg",
        "outputId": "e5a34a04-e6f3-4a22-c96c-74704395b5d3"
      },
      "source": [
        "device"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzBfoCR3KLFB"
      },
      "source": [
        "Define Train-Validation Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtgAl0QjnXy6"
      },
      "source": [
        "def train_test(num_epochs):\r\n",
        "  best_acc = 0.0\r\n",
        "\r\n",
        "  for epoch in range(num_epochs):\r\n",
        "    model.train()\r\n",
        "    train_acc = 0.0\r\n",
        "    train_loss = 0.0\r\n",
        "    for i, batch in enumerate(train_loader):\r\n",
        "      if cuda_avail:\r\n",
        "        #images = Variable(images.cuda())\r\n",
        "        #labels = Variable(labels.cuda())\r\n",
        "        inputs, targets = batch['image'].to(device,dtype=torch.float), batch['label'].to(device,dtype=torch.long)\r\n",
        "        #print(type(inputs))\r\n",
        "            # Clear all accumulated gradients\r\n",
        "      optimizer.zero_grad()\r\n",
        "            # Predict classes using images from the test set\r\n",
        "      outputs = model(inputs)\r\n",
        "            # Compute the loss based on the predictions and actual labels\r\n",
        "      loss = loss_fn(outputs, targets)\r\n",
        "            # Backpropagate the loss\r\n",
        "      loss.backward()\r\n",
        "\r\n",
        "            # Adjust parameters according to the computed gradients\r\n",
        "      optimizer.step()\r\n",
        "\r\n",
        "      train_loss += loss.cuda().data * inputs.size(0)\r\n",
        "      _, prediction = torch.max(outputs.data, 1)\r\n",
        "            \r\n",
        "      train_acc += torch.sum(prediction == targets.data)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "        # Evaluate on the test set\r\n",
        "    model.eval()\r\n",
        "\r\n",
        "    test_acc = 0.0\r\n",
        "    test_loss = 0.0\r\n",
        "    with torch.no_grad():\r\n",
        "      for b, batch in enumerate(test_loader):\r\n",
        "        inputs, targets = batch['image'].to(device,dtype=torch.float), batch['label'].to(device,dtype=torch.long)\r\n",
        "        outputs = model(inputs)\r\n",
        "        #loss = loss_fn(outputs, targets)\r\n",
        "        \r\n",
        "        # update-average-validation-loss \r\n",
        "        #test_loss += loss.cuda().data * inputs.size(0)\r\n",
        "        _, prediction = torch.max(outputs.data, 1)\r\n",
        "            \r\n",
        "        test_acc += torch.sum(prediction == targets.data)\r\n",
        "\r\n",
        "    mean_train_acc = train_acc/len(train_loader.sampler)   \r\n",
        "    mean_train_loss = train_loss/len(train_loader.sampler)\r\n",
        "    mean_test_acc = test_acc/len(test_loader.sampler)   \r\n",
        "\r\n",
        "\r\n",
        "        # Print the metrics\r\n",
        "    print(\"Epoch {}, Train Accuracy: {} , TrainLoss: {} , Test Accuracy: {}\".format(epoch+1, mean_train_acc, mean_train_loss,mean_test_acc))\r\n",
        "\r\n",
        "                        "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAeMvGr8KP_p"
      },
      "source": [
        "Train 20 epochs (or more if enough computation resources allocated)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPoTv0dKpCcj",
        "outputId": "cb32d686-d716-4e44-9e53-3bae00cbd1e5"
      },
      "source": [
        "if __name__ == \"__main__\":\r\n",
        "  train_test(20)\r\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Train Accuracy: 0.5360000133514404 , TrainLoss: 0.8234519362449646 , Test Accuracy: 0.3878205120563507\n",
            "Epoch 2, Train Accuracy: 0.6390000581741333 , TrainLoss: 0.6575329899787903 , Test Accuracy: 0.44871795177459717\n",
            "Epoch 3, Train Accuracy: 0.6810000538825989 , TrainLoss: 0.6136254072189331 , Test Accuracy: 0.5240384936332703\n",
            "Epoch 4, Train Accuracy: 0.7410000562667847 , TrainLoss: 0.5592641830444336 , Test Accuracy: 0.5\n",
            "Epoch 5, Train Accuracy: 0.7550000548362732 , TrainLoss: 0.5337328314781189 , Test Accuracy: 0.5416666865348816\n",
            "Epoch 6, Train Accuracy: 0.76500004529953 , TrainLoss: 0.5125279426574707 , Test Accuracy: 0.5480769276618958\n",
            "Epoch 7, Train Accuracy: 0.7850000262260437 , TrainLoss: 0.46403923630714417 , Test Accuracy: 0.5288461446762085\n",
            "Epoch 8, Train Accuracy: 0.7990000247955322 , TrainLoss: 0.4430633783340454 , Test Accuracy: 0.5288461446762085\n",
            "Epoch 9, Train Accuracy: 0.8020000457763672 , TrainLoss: 0.4357111155986786 , Test Accuracy: 0.5336538553237915\n",
            "Epoch 10, Train Accuracy: 0.8200000524520874 , TrainLoss: 0.40267473459243774 , Test Accuracy: 0.557692289352417\n",
            "Epoch 11, Train Accuracy: 0.8030000329017639 , TrainLoss: 0.41512060165405273 , Test Accuracy: 0.5512820482254028\n",
            "Epoch 12, Train Accuracy: 0.8240000605583191 , TrainLoss: 0.40702924132347107 , Test Accuracy: 0.5144230723381042\n",
            "Epoch 13, Train Accuracy: 0.8250000476837158 , TrainLoss: 0.37236684560775757 , Test Accuracy: 0.5384615659713745\n",
            "Epoch 14, Train Accuracy: 0.8440000414848328 , TrainLoss: 0.34968796372413635 , Test Accuracy: 0.5512820482254028\n",
            "Epoch 15, Train Accuracy: 0.8380000591278076 , TrainLoss: 0.37160566449165344 , Test Accuracy: 0.5464743375778198\n",
            "Epoch 16, Train Accuracy: 0.8580000400543213 , TrainLoss: 0.32934677600860596 , Test Accuracy: 0.5352564454078674\n",
            "Epoch 17, Train Accuracy: 0.859000027179718 , TrainLoss: 0.33392253518104553 , Test Accuracy: 0.5\n",
            "Epoch 18, Train Accuracy: 0.8600000143051147 , TrainLoss: 0.313314825296402 , Test Accuracy: 0.5336538553237915\n",
            "Epoch 19, Train Accuracy: 0.8760000467300415 , TrainLoss: 0.298622727394104 , Test Accuracy: 0.5272436141967773\n",
            "Epoch 20, Train Accuracy: 0.8790000677108765 , TrainLoss: 0.2766672968864441 , Test Accuracy: 0.5464743375778198\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}